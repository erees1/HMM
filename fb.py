import numpy as np


def HMMforward(v, Tr, p1, E, norm=True):
    """Forward step of the HMM algorithm

    Args:
        v (np.array): Observations, shape (T,)
        Tr (np.array): Transistion matrix: Tr_ij = p(ht+1 = i | ht = j)
        p1 (np.array): Initial state, shape (n,)
        E (np.array): Emission matrix: E_ij = p(vt = i | ht = j)
        norm (bool, optional): Whether to normalise at each step. Defaults to True.

    Returns:
        alpha (np.array): filtered posterior i.e. p(ht | v1:t) if norm=True,
            if norm=False then this is equal to the joint p(ht, v1:t) although
            numerical difficulties are to be excpected as this algo is not implemented
            in log space
        loglik (np.array): this is the sum of normalisation factors and corresponds to the loglik
            of the data
        z (np.array): normalization factors at each step
    """

    H = len(p1)
    T = len(v)
    assert p1.shape[0] == H
    assert E.shape[1] == H

    alpha = np.zeros((H, T))  # init normalized alpha values
    z = np.zeros((T))  # init local normalisation factors

    # p(h1, v1) = p(v1 | h1) * p(h1)
    alpha[:, 0] = E[v[0], :] * p1
    z[0] = sum(alpha[:, 0])
    if norm:
        alpha[:, 0] = alpha[:, 0] / z[0]
    for t in range(1, T):
        alpha[:, t] = E[v[t], :] * (Tr.dot(alpha[:, t - 1]))
        z[t] = sum(alpha[:, t])
        if norm:
            alpha[:, t] = alpha[:, t] / z[t]

    loglik = sum(np.log(z[:]))  # log likelihood
    return alpha, loglik, z


def HMMbackward(v, Tr, E, norm=True):
    """Backward step of the HMM algorithm

    Args:
        v (np.array): Observations, shape (T,)
        Tr (np.array): Transistion matrix: Tr_ij = p(ht+1 = i | ht = j)
        E (np.array): Emission matrix: E_ij = p(vt = i | ht = j)
        norm (bool, optional): Whether to normalise at each step. Defaults to True.

    Returns:
        beta (np.array):
    """

    H = len(Tr)
    T = len(v)
    beta = np.zeros((H, T))  # init normalized beta values
    z = np.zeros((T))  # init local normalisation factors

    beta[:, -1] = [1, 1]
    z[-1] = sum(beta[:, -1])
    if norm:
        beta[:, -1] = beta[:, -1] / z[-1]

    for t in range(1, T)[::-1]:
        beta[:, t - 1] = (E[v[t], :] * Tr.T).dot(beta[:, t])
        z[t - 1] = sum(beta[:, t - 1])
        if norm:
            beta[:, t - 1] = beta[:, t - 1] / z[t - 1]

    return beta


def HMMfb(v, Tr, E, p1):
    """Runs forward and backward step of the HMM algorithm
    to obtain the filtered posterior p(ht | v1:T)

    Args:
        v (np.array): Observations, shape (T,)
        Tr (np.array): Transistion matrix: Tr_ij = p(ht+1 = i | ht = j)
        E (np.array): Emission matrix: E_ij = p(vt = i | ht = j)
        p1 (np.array): Initial state, shape (n,)

    Returns:
        gamma_norm, alpha, beta, loglik
    """

    alpha, loglik, z = HMMforward(v, Tr, p1, E, norm=True)
    beta = HMMbackward(v, Tr, E, norm=True)
    gamma = alpha * beta
    gamma_norm = gamma / gamma.sum(axis=0)
    return gamma_norm, alpha, beta, loglik


def pairwise(t, v, alpha, beta, E, Tr):
    """Generates pairwise probabilities of HMM, p(ht+1,ht | v1:T)

    Args:
        t (int): which pair to generate
        v (np.array): Observations
        alpha (np.array): alpha generated by HMMforward
        beta (np.array): beta generated by HMMbackward
        E (np.array): Emission matrix: E_ij = p(vt = i | ht = j)
        Tr (np.array): Transistion matrix: Tr_ij = p(ht+1 = i | ht = j)

    Returns:
        np.array: Pij with p(ht+1 = i, ht = j | v1:T)
    """
    m1 = alpha[:, t].reshape(-1, 1).repeat(2, axis=1)
    m1 = m1 * Tr.T
    m2 = beta[:, t + 1] * E[v[t + 1], :]
    m2 = m2.reshape(1, -1).repeat(2, axis=0)

    # Rows correspond to ht+1 columns to ht
    m3 = m1 * m2
    return m3.T / m3.sum()
